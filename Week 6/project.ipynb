{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 6 Project\n",
        "\n",
        "This project will develop a treatment plan for a fictious illness \"Twizzleflu\".\n",
        "Twizzleflu is a mild illness caused by a virus.\n",
        "The main symptoms are a mild fever, fidgeting, and kicking the blankets off the bed or couch.\n",
        "Mild dehydration has also been reported in more severe cases.\n",
        "These symptoms typically last 1-2 weeks without treatment.\n",
        "Word on the internet says that Twizzleflu can be cured faster by drinking copious orange juice, but this has not been supported by evidence so far.\n",
        "You will be provided with a theoretical model of Twizzleflu modeled as a Markov decision process.\n",
        "Based on the model, you will compute optimal treatment plans to optimize different criteria, and compare patient discomfort with the different plans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzyRo9Tw5VcB"
      },
      "source": [
        "The full project description, a template notebook, and raw data are available on GitHub: [Project 6 Materials](https://github.com/bu-cds-dx704/dx704-project-06)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGYOZcnP6Vfu"
      },
      "source": [
        "We will model Twizzleflu as a Markov decision process.\n",
        "The model transition probabilities are provided in the file \"twizzleflu-transitions.tsv\" and the expected rewards are in \"twizzleflu-rewards.tsv\".\n",
        "The goal for Twizzleflu is to minimize the expected discomfort of the patient which is expressed as negative rewards in the file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlm2sUsades5"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Evaluate a Do Nothing Plan\n",
        "\n",
        "One of the treatment actions is to do nothing.\n",
        "Calculate the expected discomfort (not rewards) of a policy that always does nothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvG4mi_sAF9A"
      },
      "source": [
        "Hint: for this value calculation and later ones, use value iteration.\n",
        "The analytical solution has difficulties in practice when there is no discount factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVfnE8vf8yIl"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Computing expected discomfort under \"do-nothing\"\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "rewards = pd.read_csv(\"twizzleflu-rewards.tsv\", sep=\"\\t\")\n",
        "trans   = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "\n",
        "states = sorted(set(trans[\"state\"]).union(trans[\"next_state\"]))\n",
        "R = {(s, a): r for a, s, r in rewards.itertuples(index=False)}\n",
        "P = defaultdict(list)\n",
        "for a, s, s2, p in trans.itertuples(index=False):\n",
        "    P[(s, a)].append((s2, p))\n",
        "\n",
        "gamma, tol = 0.95, 1e-10\n",
        "V = {s: 0.0 for s in states}\n",
        "C = {s: -R.get((s, \"do-nothing\"), 0.0) for s in states}\n",
        "\n",
        "for _ in range(10000):\n",
        "    V_new = {s: C[s] + gamma * sum(p * V[s2] for s2, p in P[(s, \"do-nothing\")]) for s in states}\n",
        "    if max(abs(V_new[s] - V[s]) for s in states) < tol:\n",
        "        V = V_new\n",
        "        break\n",
        "    V = V_new\n",
        "\n",
        "do_nothing_df = pd.DataFrame({\"state\": states, \"expected_discomfort\": [V[s] for s in states]})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oji9gHEk8ytE"
      },
      "source": [
        "Save the expected discomfort by state to a file \"do-nothing-discomfort.tsv\" with columns state and expected_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZLDuiAb99ACA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved do-nothing-discomfort.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "do_nothing_df.to_csv(\"do-nothing-discomfort.tsv\", sep=\"\\t\", index=False)\n",
        "print(\"Saved do-nothing-discomfort.tsv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-8sGANC-Dzs"
      },
      "source": [
        "Submit \"do-nothing-discomfort.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ1ietVp9BCS"
      },
      "source": [
        "## Part 2: Compute an Optimal Treatment Plan\n",
        "\n",
        "Compute an optimal treatment plan for Twizzleflu.\n",
        "It should minimize the expected discomfort (maximize the rewards)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6fdjt6qk9mZM"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Computing optimal treatment plan (min discomfort max reward)\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "rewards = pd.read_csv(\"twizzleflu-rewards.tsv\", sep=\"\\t\")\n",
        "trans   = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "\n",
        "states  = sorted(set(trans[\"state\"]).union(trans[\"next_state\"]))\n",
        "actions = sorted(trans[\"action\"].unique())\n",
        "\n",
        "# Build R(s,a) and P(s'|s,a)\n",
        "R = {(s, a): r for a, s, r in rewards.itertuples(index=False)}\n",
        "P = defaultdict(list)\n",
        "for a, s, s2, p in trans.itertuples(index=False):\n",
        "    P[(s, a)].append((s2, p))\n",
        "\n",
        "# Value iteration (maximize expected discounted reward)\n",
        "gamma, tol, max_iters = 0.95, 1e-10, 10000\n",
        "V = {s: 0.0 for s in states}\n",
        "\n",
        "for _ in range(max_iters):\n",
        "    V_new = {\n",
        "        s: max(R.get((s,a),0.0) + gamma*sum(p*V[s2] for s2,p in P[(s,a)]) for a in actions)\n",
        "        for s in states\n",
        "    }\n",
        "    if max(abs(V_new[s] - V[s]) for s in states) < tol:\n",
        "        V = V_new\n",
        "        break\n",
        "    V = V_new\n",
        "\n",
        "# Greedy policy from V\n",
        "best = {}\n",
        "for s in states:\n",
        "    qa = [(a, R.get((s,a),0.0) + gamma*sum(p*V[s2] for s2,p in P[(s,a)])) for a in actions]\n",
        "    best[s] = max(qa, key=lambda t: t[1])[0]\n",
        "\n",
        "minimum_discomfort_actions = pd.DataFrame(\n",
        "    {\"state\": states, \"action\": [best[s] for s in states]}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRcByl1h9nBf"
      },
      "source": [
        "Save the optimal actions for each state to a file \"minimum-discomfort-actions.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FhAajvpX9wru"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved minimum-discomfort-actions.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "minimum_discomfort_actions.to_csv(\"minimum-discomfort-actions.tsv\", sep=\"\\t\", index=False)\n",
        "print(\"Saved minimum-discomfort-actions.tsv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr00MlhL-Hdv"
      },
      "source": [
        "Submit \"minimum-discomfort-actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65p3NRTy9xjT"
      },
      "source": [
        "## Part 3: Expected Discomfort\n",
        "\n",
        "Using your previous optimal policy, compute the expected discomfort for each state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t5bHbK24-AhQ"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Computing expected discomfort under the optimal policy\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Inputs\n",
        "rewards = pd.read_csv(\"twizzleflu-rewards.tsv\", sep=\"\\t\")\n",
        "trans   = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "policy  = pd.read_csv(\"minimum-discomfort-actions.tsv\", sep=\"\\t\")  # from Part 2\n",
        "\n",
        "states  = sorted(set(trans[\"state\"]).union(trans[\"next_state\"]))\n",
        "\n",
        "# Build R(s,a) and P(s'|s,a)\n",
        "R = {(s, a): r for a, s, r in rewards.itertuples(index=False)}\n",
        "P = defaultdict(list)\n",
        "for a, s, s2, p in trans.itertuples(index=False):\n",
        "    P[(s, a)].append((s2, p))\n",
        "\n",
        "# Policy mapping: a*(s)\n",
        "pi = dict(zip(policy[\"state\"], policy[\"action\"]))\n",
        "\n",
        "# Evaluate *cost* (discomfort) with value iteration on costs\n",
        "gamma, tol, max_iters = 0.95, 1e-10, 10000\n",
        "C = {s: -R.get((s, pi[s]), 0.0) for s in states}     # per-step discomfort = -reward\n",
        "V = {s: 0.0 for s in states}\n",
        "\n",
        "for _ in range(max_iters):\n",
        "    V_new = {\n",
        "        s: C[s] + gamma * sum(p * V[s2] for s2, p in P[(s, pi[s])])\n",
        "        for s in states\n",
        "    }\n",
        "    if max(abs(V_new[s] - V[s]) for s in states) < tol:\n",
        "        V = V_new\n",
        "        break\n",
        "    V = V_new\n",
        "\n",
        "minimum_discomfort_values = pd.DataFrame(\n",
        "    {\"state\": states, \"expected_discomfort\": [V[s] for s in states]}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er6-0c0f-BGw"
      },
      "source": [
        "Save your results in a file \"minimum-discomfort-values.tsv\" with columns state and expected_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NAQFQnp_-TZ1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved minimum-discomfort-values.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "minimum_discomfort_values.to_csv(\"minimum-discomfort-values.tsv\", sep=\"\\t\", index=False)\n",
        "print(\"Saved minimum-discomfort-values.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83wnFZfk-UDd"
      },
      "source": [
        "Submit \"minimum-discomfort-values.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKUTt9gx-XBF"
      },
      "source": [
        "## Part 4: Minimizing Twizzleflu Duration\n",
        "\n",
        "Modifiy the Markov decision process to minimize the days until the Twizzle flu is over.\n",
        "To do so, change the reward function to always be -1 if the current state corresponds to being sick and 0 if the current state corresponds to being better.\n",
        "To be clear, the action does not matter for this reward function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HXrnkCh5-trk"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Building duration-focused reward function\n",
        "import pandas as pd\n",
        "\n",
        "trans = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "\n",
        "states  = sorted(set(trans[\"state\"]).union(trans[\"next_state\"]))\n",
        "actions = sorted(trans[\"action\"].unique())\n",
        "\n",
        "def is_sick(s):  # only 'recovered' is better\n",
        "    return s != \"recovered\"\n",
        "\n",
        "rows = []\n",
        "for s in states:\n",
        "    r = -1 if is_sick(s) else 0\n",
        "    for a in actions:\n",
        "        rows.append({\"action\": a, \"state\": s, \"reward\": float(r)})\n",
        "\n",
        "duration_rewards = pd.DataFrame(rows, columns=[\"action\",\"state\",\"reward\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je9Rt239-uRl"
      },
      "source": [
        "Save your new reward function in a file \"duration-rewards.tsv\" in the same format as \"twizzleflu-rewards.tsv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_cmV1ewj-4-Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved duration-rewards.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "duration_rewards.to_csv(\"duration-rewards.tsv\", sep=\"\\t\", index=False)\n",
        "print(\"Saved duration-rewards.tsv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0lubs9v-5XQ"
      },
      "source": [
        "Submit \"duration-rewards.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf73YFzB-802"
      },
      "source": [
        "## Part 5: Optimize for Shorter Twizzleflu\n",
        "\n",
        "Compute an optimal policy to minimize the duration of Twizzleflu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sa_HI0f0_FHA"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Computing optimal policy for minimum duration ===\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Inputs\n",
        "trans   = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "dreward = pd.read_csv(\"duration-rewards.tsv\",      sep=\"\\t\")\n",
        "\n",
        "states  = sorted(set(trans[\"state\"]).union(trans[\"next_state\"]))\n",
        "actions = sorted(trans[\"action\"].unique())\n",
        "\n",
        "# R(s,a) and P(s'|s,a)\n",
        "R = {(s, a): r for a, s, r in dreward.itertuples(index=False)}\n",
        "P = defaultdict(list)\n",
        "for a, s, s2, p in trans.itertuples(index=False):\n",
        "    P[(s, a)].append((s2, p))\n",
        "\n",
        "# Value iteration (maximize reward = minimize expected duration)\n",
        "gamma, tol, max_iters = 0.95, 1e-10, 10000\n",
        "V = {s: 0.0 for s in states}\n",
        "\n",
        "for _ in range(max_iters):\n",
        "    V_new = {\n",
        "        s: max(R.get((s,a),0.0) + gamma*sum(p*V[s2] for s2,p in P[(s,a)]) for a in actions)\n",
        "        for s in states\n",
        "    }\n",
        "    if max(abs(V_new[s] - V[s]) for s in states) < tol:\n",
        "        V = V_new\n",
        "        break\n",
        "    V = V_new\n",
        "\n",
        "# Greedy policy\n",
        "best = {}\n",
        "for s in states:\n",
        "    qa = [(a, R.get((s,a),0.0) + gamma*sum(p*V[s2] for s2,p in P[(s,a)])) for a in actions]\n",
        "    best[s] = max(qa, key=lambda t: t[1])[0]\n",
        "\n",
        "minimum_duration_actions = pd.DataFrame({\"state\": states, \"action\": [best[s] for s in states]})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px1xDndA_F3O"
      },
      "source": [
        "Save the optimal actions for each state to a file \"minimum-duration-actions.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PGvWqSiI_Sqy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved minimum-duration-actions.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "minimum_duration_actions.to_csv(\"minimum-duration-actions.tsv\", sep=\"\\t\", index=False)\n",
        "print(\"Saved minimum-duration-actions.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itPVLMaM_UDn"
      },
      "source": [
        "Submit \"minimum-duration-actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOzSQ3fV_XBO"
      },
      "source": [
        "## Part 6: Shorter Twizzleflu?\n",
        "\n",
        "Compute the expected number of days sick for each state to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WO_yubXg_gxn"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Computing expected sick days under the minimum-duration policy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "trans  = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "policy = pd.read_csv(\"minimum-duration-actions.tsv\", sep=\"\\t\")\n",
        "\n",
        "states = sorted(set(trans[\"state\"]).union(trans[\"next_state\"]))\n",
        "pi = dict(zip(policy[\"state\"], policy[\"action\"]))\n",
        "\n",
        "def is_sick(s):  # only 'recovered' is healthy\n",
        "    return s != \"recovered\"\n",
        "\n",
        "# Focus on transient (sick) states; set v(recovered)=0 as boundary condition\n",
        "sick_states = [s for s in states if is_sick(s)]\n",
        "healthy_states = [s for s in states if not is_sick(s)]  # should be [\"recovered\"]\n",
        "\n",
        "idx_sick = {s:i for i,s in enumerate(sick_states)}\n",
        "n = len(sick_states)\n",
        "\n",
        "# Build P_pi restricted to sick->sick transitions\n",
        "Ppi_ss = np.zeros((n, n), dtype=float)\n",
        "for _, row in trans.iterrows():\n",
        "    s, a, s2, p = row[\"state\"], row[\"action\"], row[\"next_state\"], row[\"probability\"]\n",
        "    if s in idx_sick and a == pi[s] and s2 in idx_sick:\n",
        "        Ppi_ss[idx_sick[s], idx_sick[s2]] += p\n",
        "\n",
        "# Cost vector: 1 per day while sick\n",
        "c_sick = np.ones(n, dtype=float)\n",
        "\n",
        "# Solve (I - P_ss) v_sick = c_sick\n",
        "I = np.eye(n)\n",
        "try:\n",
        "    v_sick = np.linalg.solve(I - Ppi_ss, c_sick)\n",
        "except np.linalg.LinAlgError:\n",
        "    # Numerical fallback if nearly singular\n",
        "    v_sick, *_ = np.linalg.lstsq(I - Ppi_ss, c_sick, rcond=None)\n",
        "\n",
        "# Assemble full vector (recovered = 0)\n",
        "v_all = {s: 0.0 for s in states}\n",
        "for s in sick_states:\n",
        "    v_all[s] = float(v_sick[idx_sick[s]])\n",
        "\n",
        "minimum_duration_days = pd.DataFrame(\n",
        "    {\"state\": states, \"expected_sick_days\": [v_all[s] for s in states]}\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zf8j6_D_hbZ"
      },
      "source": [
        "Save the expected sick days for each state to a file \"minimum-duration-days.tsv\" with columns state and expected_sick_days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yWS2HNVl_o3P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved minimum-duration-days.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "minimum_duration_days.to_csv(\"minimum-duration-days.tsv\", sep=\"\\t\", index=False)\n",
        "print(\"Saved minimum-duration-days.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVApozXF_pjI"
      },
      "source": [
        "Submit \"minimum-duration-days.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znw87KK7_uv5"
      },
      "source": [
        "## Part 7: Speed vs Pampering\n",
        "\n",
        "Compute the expected discomfort using the policy to minimize days sick, and compare the results to the expected discomfort when optimizing to minimize discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0AdnpD-6__y5"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Computing expected discomfort under two policies (speed vs pampering)\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Inputs\n",
        "rewards = pd.read_csv(\"twizzleflu-rewards.tsv\", sep=\"\\t\")              # original rewards\n",
        "trans   = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "pi_speed = pd.read_csv(\"minimum-duration-actions.tsv\", sep=\"\\t\")       # Part 5\n",
        "pi_comfort = pd.read_csv(\"minimum-discomfort-actions.tsv\", sep=\"\\t\")   # Part 2\n",
        "\n",
        "states  = sorted(set(trans[\"state\"]).union(trans[\"next_state\"]))\n",
        "\n",
        "# Build R(s,a) and P(s'|s,a)\n",
        "R = {(s, a): r for a, s, r in rewards.itertuples(index=False)}\n",
        "P = defaultdict(list)\n",
        "for a, s, s2, p in trans.itertuples(index=False):\n",
        "    P[(s, a)].append((s2, p))\n",
        "\n",
        "# Policy maps\n",
        "pi_speed_map   = dict(zip(pi_speed[\"state\"],   pi_speed[\"action\"]))\n",
        "pi_comfort_map = dict(zip(pi_comfort[\"state\"], pi_comfort[\"action\"]))\n",
        "\n",
        "# Evaluate expected DISCOMFORT (negative reward) for a fixed policy using value iteration\n",
        "def eval_discomfort(pi_map, gamma=0.95, tol=1e-10, max_iters=10000):\n",
        "    C = {s: -R.get((s, pi_map[s]), 0.0) for s in states}  # per-step discomfort\n",
        "    V = {s: 0.0 for s in states}\n",
        "    for _ in range(max_iters):\n",
        "        V_new = {\n",
        "            s: C[s] + gamma * sum(p * V[s2] for s2, p in P[(s, pi_map[s])])\n",
        "            for s in states\n",
        "        }\n",
        "        if max(abs(V_new[s] - V[s]) for s in states) < tol:\n",
        "            V = V_new\n",
        "            break\n",
        "        V = V_new\n",
        "    return V\n",
        "\n",
        "V_speed   = eval_discomfort(pi_speed_map)     # expected discomfort under min-duration policy\n",
        "V_comfort = eval_discomfort(pi_comfort_map)   # expected discomfort under min-discomfort policy\n",
        "\n",
        "policy_comparison = pd.DataFrame({\n",
        "    \"state\": states,\n",
        "    \"speed_discomfort\":   [V_speed[s] for s in states],\n",
        "    \"minimize_discomfort\":[V_comfort[s] for s in states],\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ZVJ2lcAAkP"
      },
      "source": [
        "Save the results to a file \"policy-comparison.tsv\" with columns state, speed_discomfort, and minimize_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9H9EG0zTAMt1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved policy-comparison.tsv\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "policy_comparison.to_csv(\"policy-comparison.tsv\", sep=\"\\t\", index=False)\n",
        "print(\"Saved policy-comparison.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVhLZuuaANNf"
      },
      "source": [
        "Submit \"policy-comparison.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 8: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 9: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "ack_text = \"\"\"Acknowledgments\n",
        "\n",
        "I used ChatGPT as a study partner to clarify  and check errors, and simplify code while completing this project. \n",
        "\n",
        "\n",
        "For transparency, I am including links to the transcripts of these interactions:\n",
        "\n",
        "1. ChatGPT guidance: [https://chatgpt.com/share/68d20a8d-0870-8007-bb79-080efab54c33]\n",
        "These resources document the support I received and ensure compliance with the class generative AI policy.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"acknowledgments.txt\",\"w\") as f:\n",
        "    f.write(ack_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
